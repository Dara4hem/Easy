# Synthetic Review Generator with Quality Guardrails

A production-ready synthetic data generation system for developer tool reviews with automated quality validation, multi-model support, and comprehensive evaluation against real data.

## üéØ Overview

This project generates realistic synthetic reviews for developer tools (Postman, Insomnia, Hoppscotch) using multiple LLM providers and validates them against strict quality guardrails. The system includes:

- **500 synthetic reviews** (200 OpenAI + 300 Qwen3-30B)
- **50 real G2 reviews** as ground truth
- **Multi-model comparison** (OpenAI gpt-4.1-mini vs local Qwen3-30B)
- **Automated quality guardrails** with rejection/regeneration
- **Comprehensive metrics**: diversity, bias detection, domain realism, sentiment analysis

### Why Dev Tools & Postman?

Developer tools represent a domain with:
- **Rich technical vocabulary** (API, CI/CD, auth, endpoints, debugging)
- **Diverse user personas** (junior/senior engineers, QA specialists)
- **Clear use cases** that ground reviews in realistic workflows
- **Available real data** from review platforms like G2

This makes them ideal for demonstrating synthetic data quality and realism.

---

## üì¶ Setup

### Prerequisites

- **Python 3.10+**
- **OpenAI API key** (for OpenAI model)
- **Ollama** (for local Qwen3-30B model)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/Dara4hem/Easy.git
cd Easy
```

2. **Install Python dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up OpenAI API key**
```bash
cp secrets.example.env .env
# Edit .env and add your OpenAI API key:
# OPENAI_API_KEY=sk-...
```

4. **Install and configure Ollama** (for local model)
```bash
# Install Ollama from https://ollama.ai/

# Pull the Qwen3-30B model (3-bit quantized, ~12GB)
ollama pull hf.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q3_K_S

# Verify Ollama is running
curl http://localhost:11434/api/tags
```

---

## üöÄ Usage

### Quick Start (5 reviews in ~10 seconds)

Test the complete pipeline with a small sample:

```bash
# Generate 5 reviews with OpenAI (with guardrails)
python -m src.synthetic_reviews.cli \
  --provider openai \
  --num-reviews 5 \
  --guardrails \
  --output data/synthetic/quick_test.jsonl

# Expected output:
# Generation time: ~10s total, ~2.0s per review
# Total attempts: 5, accepted: 5, rejection rate: 0.0%
```

### 1. Generate Synthetic Reviews

**OpenAI (gpt-4.1-mini):**
```bash
python -m src.synthetic_reviews.cli \
  --config configs/dev_tools.yaml \
  --provider openai \
  --model-name gpt-4.1-mini \
  --num-reviews 200 \
  --output data/synthetic/dev_tools_openai.jsonl
```

**Qwen3-30B (local Ollama):**
```bash
python -m src.synthetic_reviews.cli \
  --config configs/dev_tools.yaml \
  --provider ollama \
  --model-name "hf.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q3_K_S" \
  --num-reviews 300 \
  --output data/synthetic/dev_tools_qwen30b.jsonl
```

**With automated guardrails** (rejection/regeneration):
```bash
python -m src.synthetic_reviews.cli \
  --provider openai \
  --num-reviews 200 \
  --guardrails \
  --output data/synthetic/dev_tools_openai.jsonl
```

### 2. Score Generated Reviews

```bash
python -m src.synthetic_reviews.quality_cli \
  --synthetic data/synthetic/dev_tools_openai.jsonl \
  --output data/synthetic/dev_tools_openai_scored.jsonl

python -m src.synthetic_reviews.quality_cli \
  --synthetic data/synthetic/dev_tools_qwen30b.jsonl \
  --output data/synthetic/dev_tools_qwen30b_scored.jsonl
```

### 3. Generate Quality Reports

**Per-model report:**
```bash
python -m src.synthetic_reviews.quality_cli \
  --synthetic data/synthetic/dev_tools_openai_scored.jsonl \
  --label openai \
  --output reports/dev_tools_openai_quality.md
```

**Convert real data and generate comprehensive comparison:**
```bash
# Convert real G2 reviews to Review schema
python scripts/convert_real_to_reviews.py \
  --input data/real/postman_g2_reviews.jsonl \
  --output data/real/postman_g2_as_reviews.jsonl

# Generate 3-way comparison report
python -m src.synthetic_reviews.compare_cli \
  --openai data/synthetic/dev_tools_openai_scored.jsonl \
  --qwen data/synthetic/dev_tools_qwen30b_scored.jsonl \
  --real data/real/postman_g2_as_reviews.jsonl \
  --output reports/comprehensive_comparison.md
```

### 4. Scrape Real Reviews (Optional)

If you want to collect fresh real reviews:
```bash
python scripts/scrape_g2_postman.py \
  --urls-file data/real/postman_g2_urls.txt \
  --output data/real/postman_g2_reviews.jsonl
```

---

## üèóÔ∏è Architecture

### Configuration (`configs/dev_tools.yaml`)

```yaml
domain: "dev_tools"
products:
  - "Postman"
  - "Insomnia"
  - "Hoppscotch"

personas:
  - id: backend_junior
    role: "Junior Backend Engineer"
    tone: "casual"
    rating_range: [3, 5]
  
  - id: backend_senior
    role: "Senior Backend Engineer"
    tone: "direct"
    rating_range: [2, 5]
  
  - id: qa_engineer
    role: "QA Engineer"
    tone: "detailed"
    rating_range: [1, 4]

rating_distribution:
  5: 0.35
  4: 0.40
  3: 0.20
  2: 0.04
  1: 0.01
```

### Quality Metrics

**Diversity Metrics:**
- **Vocabulary Diversity**: `unique_tokens / total_tokens` per review
- **Semantic Novelty**: `1 - max_jaccard_similarity` with previous reviews
- **Domain Realism**: Keyword overlap with dev-tool terminology

**Bias/Skew Detection:**
- **Rating Distribution Skew**: Squared difference from expected distribution
- **High/Low Rating Ratios**: Detect unrealistic positivity/negativity
- **Sentiment Score**: Lexicon-based `(#positive - #negative) / total`
- **Rating-Text Mismatch**: Flags (e.g., "crash" in 5‚òÖ reviews)

**Quality Flags:**
- `too_short`: < 30 tokens
- `high_overlap`: Jaccard > 0.8 with existing reviews
- `low_domain_realism`: < 5% keyword coverage
- `rating_text_mismatch`: Sentiment conflicts with rating

### Automated Guardrails

When using `--guardrails`, the system:
1. Generates a candidate review
2. Runs lightweight quality checks
3. **Rejects** if any flags are triggered
4. **Regenerates** up to 3 times per sample
5. Tracks rejection rate and total attempts

---

## üìä Results Summary

### Dataset Sizes

| Dataset | Model | Count | Source |
|---------|-------|-------|--------|
| Synthetic (OpenAI) | gpt-4.1-mini | 200 | OpenAI API |
| Synthetic (Qwen) | Qwen3-30B (Q3_K_S) | 300 | Local Ollama |
| Real | G2 reviews | 50 | Web scraping |

### Quality Metrics (from existing runs)

| Metric | OpenAI | Qwen3-30B | Real (G2) |
|--------|--------|-----------|-----------|
| Vocab Diversity | 0.841 | 0.799 | ~0.75-0.85 (est.) |
| Semantic Novelty | 0.638 | 0.619 | ~0.60-0.70 (est.) |
| Domain Realism | 0.254 | 0.396 | Higher (real domain) |
| Rejection Rate | 1.0% | 1.7% | 0% (ground truth) |
| Avg Rating | 4.14 | 4.35 | 4.65 |

**Key Findings:**
- **Qwen3-30B** produces more domain-realistic reviews (0.396 vs 0.254) with concrete dev-tool workflows
- **OpenAI** has slightly better vocabulary diversity but more generic "SaaS review" tone
- Both models maintain realistic rating distributions close to config targets
- Low rejection rates (1-2%) indicate high base quality from both models

### Performance Benchmarks

Measured on the above configuration:

- **OpenAI gpt-4.1-mini**: **~2.0s per review** (measured with guardrails)
  - Fast, consistent API latency
  - Cost: ~$0.001 per review (~$0.20 for 200 reviews)
  
- **Qwen3-30B (local Ollama)**: ~5-8s per review on CPU
  - Free, fully local execution
  - Model size: ~12GB (Q3_K_S quantization)
  - With GPU acceleration: Could improve to ~2-3s per review
  - *Note: Timing estimated based on model size and hardware; actual data generated offline*
  
- **Quality scoring**: ~0.01s per review (very fast)
- **Real data conversion**: ~0.004s per review
- **Comparison report generation**: ~0.003s per review

---

## üé® Design Decisions & Trade-offs

### 1. **Why Two Models?**

- **OpenAI (gpt-4.1-mini)**: High-quality baseline, fast via API, but costs $$ and requires internet
- **Qwen3-30B (Ollama)**: Free, local, more domain-realistic for dev tools, but slower and requires ~12GB RAM

**Trade-off**: Cost vs control. OpenAI for rapid prototyping, Qwen for production scale.

### 2. **Why Rule-Based Guardrails?**

Instead of training a classifier:
- **Simplicity**: No labeled training data needed
- **Transparency**: Easy to inspect and tune thresholds
- **Speed**: O(n) complexity, works in real-time during generation

**Trade-off**: Less nuanced than ML models, but sufficient for catching common issues.

### 3. **Why Jaccard for Semantic Similarity?**

Instead of embeddings (SBERT, OpenAI embeddings):
- **No extra API calls** or heavy models
- **Fast**: O(n) token set operations
- **Interpretable**: Direct lexical overlap

**Trade-off**: Misses paraphrases, but effective at catching template reuse.

### 4. **Why Dev Tools Domain?**

- **Rich technical vocabulary** makes quality/realism easier to measure
- **Clear personas** (junior/senior devs, QA) produce diverse tones
- **Real data availability** from G2, Capterra, etc.

**Trade-off**: Less generalizable to non-technical domains without retraining/reconfiguring.

---

## üñ•Ô∏è Hardware & Model Limitations

### Tested Configuration

- **OS**: Windows 11 (Build 26100)
- **CPU**: Intel Core i7-13650HX (13th Gen)
- **RAM**: 16GB
- **GPU**: NVIDIA GeForce RTX 4060 Laptop GPU
- **Python**: 3.12.10
- **Storage**: 50GB+ free space for Ollama models

### Ollama/Qwen3-30B Notes

- **Model size**: ~12GB (Q3_K_S quantization)
- **RAM requirement**: 16GB+ recommended (32GB ideal)
- **Generation speed**:
  - CPU-only: ~5-8s per review
  - With GPU: ~2-3s per review
- **Warmup**: First request takes ~10-15s (model loading)

### OpenAI API

- **Rate limits**: 60 requests/min on free tier, 3500/min on paid
- **Cost**: ~$0.001 per review (200 reviews ‚âà $0.20)
- **Latency**: 1-3s per request (network dependent)

### Scaling Considerations

- **To generate 10k+ reviews**: Use Ollama locally to avoid API costs
- **For faster iteration**: Use OpenAI during development, switch to Qwen for production
- **To reduce memory**: Use smaller Qwen models (8B) or higher quantization (Q4, Q5)

---

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ dev_tools.yaml           # Domain, personas, rating distribution
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ real/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postman_g2_reviews.jsonl       # 50 real G2 reviews (raw schema)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ postman_g2_as_reviews.jsonl    # Converted to Review schema
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ postman_g2_urls.txt            # G2 URLs for scraping
‚îÇ   ‚îî‚îÄ‚îÄ synthetic/
‚îÇ       ‚îú‚îÄ‚îÄ dev_tools_openai.jsonl         # 200 OpenAI reviews (unscored)
‚îÇ       ‚îú‚îÄ‚îÄ dev_tools_openai_scored.jsonl  # With quality metrics
‚îÇ       ‚îú‚îÄ‚îÄ dev_tools_qwen30b.jsonl        # 300 Qwen reviews (unscored)
‚îÇ       ‚îî‚îÄ‚îÄ dev_tools_qwen30b_scored.jsonl # With quality metrics
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ dev_tools_openai_quality.md        # Per-model quality report
‚îÇ   ‚îú‚îÄ‚îÄ dev_tools_qwen30b_quality.md
‚îÇ   ‚îî‚îÄ‚îÄ comprehensive_comparison.md        # 3-way comparison
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ scrape_g2_postman.py               # Scrape real reviews from G2
‚îÇ   ‚îú‚îÄ‚îÄ analyze_real_postman.py            # Basic stats on real reviews
‚îÇ   ‚îî‚îÄ‚îÄ convert_real_to_reviews.py         # Convert real data to Review schema
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ synthetic_reviews/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ cli.py                         # Main generation CLI
‚îÇ       ‚îú‚îÄ‚îÄ config.py                      # YAML config loader
‚îÇ       ‚îú‚îÄ‚îÄ generation.py                  # OpenAI/Ollama/stub generators + guardrails
‚îÇ       ‚îú‚îÄ‚îÄ io.py                          # Load/save JSONL
‚îÇ       ‚îú‚îÄ‚îÄ quality.py                     # Quality metrics & annotate_quality()
‚îÇ       ‚îú‚îÄ‚îÄ quality_cli.py                 # Quality scoring CLI
‚îÇ       ‚îî‚îÄ‚îÄ compare_cli.py                 # Comprehensive comparison CLI
‚îú‚îÄ‚îÄ .env                                   # Your secrets (git-ignored)
‚îú‚îÄ‚îÄ secrets.example.env                    # Template for .env
‚îú‚îÄ‚îÄ requirements.txt                       # Python dependencies
‚îî‚îÄ‚îÄ README.md                              # This file
```

---

## üéì Assignment Deliverables Checklist

### ‚úÖ Generation Requirements

- [x] **300-500 samples**: 500 total (200 OpenAI + 300 Qwen3-30B)
- [x] **Configurable via YAML**: `configs/dev_tools.yaml` (personas, rating dist, products)
- [x] **2+ models/providers**: OpenAI gpt-4.1-mini + Qwen3-30B (Ollama)
- [x] **CLI interface**: `python -m src.synthetic_reviews.cli`

### ‚úÖ Quality Guardrails

- [x] **Diversity metrics**: Vocabulary diversity, semantic novelty (Jaccard-based)
- [x] **Bias detection**: Rating skew, sentiment analysis, high/low rating ratios
- [x] **Domain realism**: Dev-tool keyword overlap validation
- [x] **Automated rejection/regeneration**: `--guardrails` flag with retry logic

### ‚úÖ Engineering

- [x] **CLI interface**: Multiple CLIs (generation, scoring, comparison)
- [x] **Quality reports**: Markdown reports with metrics per model
- [x] **30-50 real reviews**: 50 G2 Postman reviews scraped and converted
- [x] **Track quality/time**: Timing per model, quality metrics in reports

### ‚úÖ Deliverables

- [x] **GitHub repo**: Clean structure, src/, configs/, data/, reports/
- [x] **Generated datasets with scores**: `*_scored.jsonl` files with quality fields
- [x] **Quality report**: Per-model + comprehensive 3-way comparison
- [x] **README**: Setup, design decisions, hardware limitations (this file)

---

## üî¨ Future Improvements

1. **Embedding-based similarity** (SBERT) for more nuanced semantic detection
2. **Multi-domain support** (SaaS, design tools, etc.) via config
3. **Web UI** for interactive generation and quality inspection
4. **A/B testing framework** to evaluate synthetic data utility in downstream tasks
5. **Persona-aware quality metrics** (e.g., junior devs should use simpler language)

---

## üìÑ License

MIT

---

## üôè Acknowledgments

- **OpenAI** for gpt-4.1-mini API
- **Ollama** for local LLM inference
- **Qwen team** for Qwen3-30B models
- **G2** for publicly available review data

---

**Generated with ‚ù§Ô∏è for production-grade synthetic data**
