import json
import random
import time
import uuid
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import typer

import requests
from dotenv import load_dotenv
from openai import OpenAI, RateLimitError
from pydantic import BaseModel, Field

from .config import Config, Persona

load_dotenv()

client = OpenAI()


class QualityScores(BaseModel):
    vocab_diversity: Optional[float] = None
    semantic_novelty: Optional[float] = None
    domain_realism: Optional[float] = None
    bias_flags: List[str] = Field(default_factory=list)
    is_accepted: Optional[bool] = None


class Review(BaseModel):
    review_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    persona_id: str
    product: str
    rating: int
    title: str
    body: str
    created_at: datetime = Field(default_factory=datetime.utcnow)
    source_model: str = "stub"
    quality: QualityScores = Field(default_factory=QualityScores)


def _sample_rating(rating_distribution: Dict[int, float]) -> int:
    ratings, probs = zip(*sorted(rating_distribution.items()))
    return random.choices(ratings, weights=probs, k=1)[0]


def _sample_persona(cfg: Config) -> Persona:
    return random.choice(cfg.personas)


# ---------- STUB GENERATION ----------

def generate_review_stub(cfg: Config, source_model: str = "stub") -> Review:

    persona = _sample_persona(cfg)
    product = random.choice(cfg.products)
    rating = _sample_rating(cfg.rating_distribution)

    title = f"Stub review for {product} ({rating}★)"
    body = (
        f"This is a placeholder review written by persona '{persona.id}' "
        f"about {product} with rating {rating}. "
        "Later this text will be generated by an LLM."
    )

    return Review(
        persona_id=persona.id,
        product=product,
        rating=rating,
        title=title,
        body=body,
        source_model=source_model,
    )


def generate_many_stub(cfg: Config, n: int) -> Tuple[List[Review], float]:
    """
    Generate n stub reviews with a simple progress bar.
    Returns (reviews, elapsed_seconds).
    """
    reviews: List[Review] = []
    start_time = time.perf_counter()
    with typer.progressbar(range(n), label=f"Stub ({n} reviews)") as progress:
        for _ in progress:
            reviews.append(generate_review_stub(cfg))
    elapsed = time.perf_counter() - start_time
    return reviews, elapsed



# ---------- COMMON PROMPT ----------

def _build_prompt(persona: Persona, product: str, rating: int) -> str:
    return f"""
You are generating a realistic user review for a dev tool.

Persona:
- Role: {persona.role}
- Tone: {persona.tone}

Product: {product}
Rating: {rating} / 5

Requirements:
- Mention at least one concrete use case or workflow (e.g. debugging APIs, running tests in CI).
- Be consistent with the rating (low rating -> more negatives, high rating -> more positives).
- Length: 60–120 words.
- Avoid generic marketing buzzwords like "revolutionary" unless they sound natural.

Output ONLY valid JSON with this exact schema:
{{
  "title": "...",
  "body": "..."
}}
""".strip()


# ---------- OPENAI GENERATION ----------

def generate_review_openai(cfg: Config, model: str = "gpt-4.1-mini") -> Review:
    persona = _sample_persona(cfg)
    product = random.choice(cfg.products)
    rating = _sample_rating(cfg.rating_distribution)

    prompt = _build_prompt(persona, product, rating)

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "You generate realistic SaaS/dev-tools product reviews as structured JSON.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.9,
        )
    except RateLimitError:
        return generate_review_stub(cfg, source_model=f"openai:{model}:rate_limited")

    content = response.choices[0].message.content

    if isinstance(content, list):
        text_parts = []
        for part in content:
            if hasattr(part, "text") and part.text is not None:
                text_parts.append(part.text)
        content = "".join(text_parts)

    if not isinstance(content, str):
        content = str(content)

    try:
        data = json.loads(content)
        title = data.get("title", f"Review for {product}")
        body = data.get("body", content)
    except json.JSONDecodeError:
        title = f"Review for {product}"
        body = content

    return Review(
        persona_id=persona.id,
        product=product,
        rating=rating,
        title=title,
        body=body,
        source_model=f"openai:{model}",
    )


def generate_many_openai(
    cfg: Config,
    n: int,
    model: str = "gpt-4.1-mini",
) -> Tuple[List[Review], float]:
    """
    Generate n reviews using OpenAI, with a progress bar.
    Returns (reviews, elapsed_seconds).
    """
    reviews: List[Review] = []
    label = f"OpenAI ({model})"
    start_time = time.perf_counter()
    with typer.progressbar(range(n), label=label) as progress:
        for _ in progress:
            reviews.append(generate_review_openai(cfg, model=model))
    elapsed = time.perf_counter() - start_time
    return reviews, elapsed



# ---------- OLLAMA (QWEN3-30B) GENERATION ----------

def generate_review_ollama(
    cfg: Config,
    model: str = "hf.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q3_K_S",
) -> Review:
    """
    Generate a review using local Ollama model (Qwen3-30B UnsLo th GGUF).
    """
    persona = _sample_persona(cfg)
    product = random.choice(cfg.products)
    rating = _sample_rating(cfg.rating_distribution)

    prompt = _build_prompt(persona, product, rating)

    payload = {
        "model": model,
        "prompt": prompt,
        "format": "json",   
        "stream": False,
        "options": {
            "temperature": 0.9,
            "num_predict": 256,
        },
    }

    resp = requests.post(
        "http://localhost:11434/api/generate",
        json=payload,
        timeout=300,
    )
    resp.raise_for_status()
    data = resp.json()
    content = data.get("response", "")

    if not isinstance(content, str):
        content = str(content)

    try:
        parsed = json.loads(content)
        title = parsed.get("title", f"Review for {product}")
        body = parsed.get("body", content)
    except json.JSONDecodeError:
        title = f"Review for {product}"
        body = content

    return Review(
        persona_id=persona.id,
        product=product,
        rating=rating,
        title=title,
        body=body,
        source_model=f"ollama:{model}",
    )


def generate_many_ollama(
    cfg: Config,
    n: int,
    model: str = "hf.co/unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q3_K_S",
) -> Tuple[List[Review], float]:
    """
    Generate n reviews using a local Ollama model, with a progress bar.
    Returns (reviews, elapsed_seconds).
    """
    reviews: List[Review] = []
    label = f"Ollama ({model})"
    start_time = time.perf_counter()
    with typer.progressbar(range(n), label=label) as progress:
        for _ in progress:
            reviews.append(generate_review_ollama(cfg, model=model))
    elapsed = time.perf_counter() - start_time
    return reviews, elapsed


# ---------- REJECTION / REGENERATION LOOP ----------

def _quick_quality_check(review: Review, seen_token_sets: List[set], domain_keywords: set) -> bool:
    """
    Lightweight quality check to see if a single review should be accepted.
    Returns True if acceptable, False if should be rejected.
    """
    from .quality import tokenize, jaccard
    
    tokens = tokenize(review.body)
    token_set = set(tokens)
    total_tokens = len(tokens)
    
    # Check 1: too short
    if total_tokens < 30:
        return False
    
    # Check 2: high overlap with existing reviews
    if seen_token_sets:
        max_j = max(jaccard(token_set, prev) for prev in seen_token_sets)
        if max_j > 0.8:
            return False
    
    # Check 3: low domain realism
    if domain_keywords:
        overlap = len(token_set & domain_keywords)
        realism = overlap / len(domain_keywords)
        if realism < 0.05:
            return False
    
    # Check 4: rating-text mismatch
    negative_words = {"bug", "crash", "fail", "broken", "unusable"}
    if review.rating >= 4 and (token_set & negative_words):
        return False
    
    positive_hype = {"perfect", "flawless", "amazing", "incredible"}
    if review.rating <= 2 and (token_set & positive_hype):
        return False
    
    return True


def generate_many_with_guardrails(
    cfg: Config,
    n: int,
    provider: str,
    model: str,
    max_tries_per_sample: int = 3,
    domain_keywords: Optional[set] = None,
) -> Tuple[List[Review], float, int]:
    """
    Generate n accepted reviews with quality guardrails and auto-regeneration.
    
    Args:
        cfg: Config object
        n: Number of accepted reviews to generate
        provider: 'openai', 'ollama', or 'stub'
        model: Model name/identifier
        max_tries_per_sample: Max regeneration attempts per sample
        domain_keywords: Set of domain keywords for realism checks
    
    Returns:
        (reviews, elapsed_seconds, total_attempts)
    """
    if domain_keywords is None:
        domain_keywords = {
            "api", "endpoint", "request", "response", "debug", "debugging",
            "auth", "token", "rest", "http", "headers", "payload", "json",
            "postman", "insomnia", "hoppscotch", "ci", "pipeline", "test", "testing"
        }
    
    # Select generator function
    if provider == "openai":
        generator = lambda: generate_review_openai(cfg, model=model)
    elif provider == "ollama":
        generator = lambda: generate_review_ollama(cfg, model=model)
    elif provider == "stub":
        generator = lambda: generate_review_stub(cfg, source_model="stub")
    else:
        raise ValueError(f"Unknown provider: {provider}")
    
    accepted_reviews: List[Review] = []
    seen_token_sets: List[set] = []
    total_attempts = 0
    
    start_time = time.perf_counter()
    label = f"{provider} (with guardrails)"
    
    with typer.progressbar(range(n), label=label) as progress:
        for _ in progress:
            tries = 0
            accepted = False
            
            while tries < max_tries_per_sample and not accepted:
                review = generator()
                total_attempts += 1
                tries += 1
                
                if _quick_quality_check(review, seen_token_sets, domain_keywords):
                    accepted_reviews.append(review)
                    from .quality import tokenize
                    seen_token_sets.append(set(tokenize(review.body)))
                    accepted = True
            
            # If we exhausted retries, accept the last one anyway to avoid infinite loops
            if not accepted:
                accepted_reviews.append(review)
                from .quality import tokenize
                seen_token_sets.append(set(tokenize(review.body)))
    
    elapsed = time.perf_counter() - start_time
    return accepted_reviews, elapsed, total_attempts

